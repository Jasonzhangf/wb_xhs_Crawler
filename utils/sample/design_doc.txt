### 1. When the task type is "wb_user":
1. Capture the content of the object with `class="ProfileHeader_name_1KbBs"` as the username. Name this crawling task as the username + date + number of items. Create a local folder with the task name, and all the crawled content will be stored in this task folder.
2. The posts with `class="Feed_body_3R0rO"` are the main element objects for crawling. For each captured object of this kind, create a new sub - folder under the task folder in the way of the current folder name's serial number + 1, and crawl the main content within it. Note that when the task starts, look for the historical record file in this directory. If it doesn't exist, create one. If it exists, read the historical record to check the folders. The historical record file will record the visited URLs and the names of the crawled folders. So, iterate through the historical record to check whether the folder name corresponding to the URL exists. If not, delete the URL's historical record. At this time, iterate through the folders, find the folder with the largest serial number, and get the current naming serial number. Before the number of crawled items is no more than the maximum `max_items`, create a new folder for each captured object by increasing the current naming serial number by 1 to crawl the object.
3. After capturing the object, here are the processes for some internal elements:
    - For the element with `class="expand"`, simulate a click to expand its content. Use the `page.evaluate` method to directly execute JavaScript code on the page to expand the content, and achieve the display of the full content by modifying the attributes and styles of the DOM elements.
    - The element with `class="detail_wbtext_4CRf9"` is the text body, which needs to be crawled as text into the JSON.
    - The post's publication time, e.g., `title="2025 - 04 - 15 09:24"`, needs to be crawled as text into the JSON.
    - The post link starting with `href="https://weibo.com/"`, like `href="https://weibo.com/1989660417/Pnl3HboaV"`, needs to be crawled as text into the JSON and the URL of the historical record.
    - The video link, e.g., `<div class="wbpv - poster" aria - disabled="false" style="background - image: url(&quot;https://wx3.sinaimg.cn/orj480/002aEpz3ly1i0gp6yh54cj60u01hcjt302.jpg&quot;);"></div>`, needs to be crawled as text into the JSON.
    - The image, e.g., `<img src="https://wx3.sinaimg.cn/orj360/002aEpz3ly1i0gn8zvs0nj60u00qwn3002.jpg" class="picture_focusImg_1z5In" style="height: 100%; width: 100%; left: 0px;">`, should be crawled into the post folder.
4. After all elements of the current post are captured, according to the `noimage` setting of the task, if `noimage` is `true`, skip the OCR process. If `noimage` is `false`, perform text extraction via `easyocr` and merge the content into the JSON of the current post.
5. After all capturable objects in the current view are captured, simulate mouse scrolling down to discover more capturable objects and capture them until the bottom of the page.
6. When the content of all posts is crawled, iterate through all folders under the task folder, merge the JSON files in them, create a file with the task folder name in the root directory of the task folder, with the merged content and the file extension `.txt`. If there is an `--export` parameter, copy this file to the directory specified after `export` and rename it to `.md`.

### 2. When the task type is "wb_keyword":
1. Name this crawling task as keyword + date + number of items. Create a local folder with the task name, and all the crawled content will be stored in this task folder.
2. Conduct the search by constructing a link similar to the following: `https://s.weibo.com/weibo?q=%E8%83%A1%E9%94%A1%E8%BF%9B&scope=ori&suball=1&timescope=custom%3A2025 - 04 - 07%3A2025 - 04 - 15&Refer=g&page=2`. Here:
    - In `q=%E8%83%A1%E9%94%A1%E8%BF%9B`, `%E8%83%A1%E9%94%A1%E8%BF%9B` is the keyword.
    - `page = 2` refers to the second page.
    - `%3A2025 - 04 - 07%3A2025 - 04 - 15` is the start time of the search. If the task doesn't specify the time, the end time is the current search time, and the start time is the current time minus 7 days.
3. After the search page stabilizes, capture the elements on the page. The posts with `class="card - feed"` are the main element objects for crawling. For each captured object of this kind, create a new sub - folder under the task folder in the way of the current folder name's serial number + 1, and crawl the main content within it. Note that when the task starts, look for the historical record file in this directory. If it doesn't exist, create one. If it exists, read the historical record to check the folders. The historical record file will record the visited URLs and the names of the crawled folders. So, iterate through the historical record to check whether the folder name corresponding to the URL exists. If not, delete the URL's historical record. At this time, iterate through the folders, find the folder with the largest serial number, and get the current naming serial number. Before the number of crawled items is no more than the maximum `max_items`, create a new folder for each captured object by increasing the current naming serial number by 1 to crawl the object.
4. After capturing the object, here are the processes for some internal elements:
    - The main content structure with `node - type="feed_list_content"` is the main content to be captured. It is the main carrier of text and needs to be crawled as text into the JSON.
    - The element with `node - type="fl_pic_list"` is an image and needs to be crawled locally.
    - The element with `class="card - act"` is for user operations. When `--comment` is `true`, simulate a click on the element with `action - type="feed_list_comment"` under it to expand the comments. After waiting for 1 second, capture the content of `class="card - review s - ptb10"` in `node - type="feed_list_repeat"`.
5. After all elements of the current post are captured, according to the `noimage` setting of the task, if `noimage` is `true`, skip the OCR process. If `noimage` is `false`, perform text extraction via `easyocr` and merge the content into the JSON of the current post.
6. After all capturable objects in the current view are captured, simulate mouse scrolling down to discover more capturable objects and capture them until the bottom of the page.
7. After capturing the element with `class="next"` at the bottom of the page, you can construct the next - page link by modifying the `page=` variable in the search link and access the next page.
8. When the content of all posts is crawled, iterate through all folders under the task folder, merge the JSON files in them, create a file with the task folder name in the root directory of the task folder, with the merged content and the file extension `.txt`. If there is an `--export` parameter, copy this file to the directory specified after `export` and rename it to `.md`.

### 3. When the task type is "xhs_keyword":
1. Name this crawling task as keyword + date + number of items. Create a local folder with the task name, and all the crawled content will be stored in this task folder.
2. The search link is `https://www.xiaohongshu.com/search_result?keyword=mcp&source=unknown`, where `keyword=mcp` means `mcp` is the keyword.
3. Capture the element with `data - v - da963056`, find the element with `id="image"` within it, and simulate a click to enter the list of graphic posts.
4. Capture the elements with `data - v - a264b01a class="title"`, check the number and position of the capturable elements in the view, click these elements in order to expand the posts. After capturing the content, simulate pressing the `Esc` key to exit the post. After all elements in the view are clicked, simulate scrolling to enter the next page and capture more elements.
5. After clicking into a post, here are the processes for some internal elements:
    - The element with `data - v - 610be4fa id="detail - title" class="title"` is the title, which needs to be crawled as a title into the local JSON.
    - The element with `data - v - 610be4fa class="note - text"` is the main body, which needs to be crawled into the local JSON.
    - The image, e.g., `<img data - v - a264b01a="" src="https://sns - webpic - qc.xhscdn.com/202504151059/31140e8c1b32078b5df9d6a1e45818dd/1040g00831fllpgebnq5g5nph1vag97flnl2hpp0!nc_n_webp_mw_1" fetchpriority="auto" loading="lazy" decoding="async" data - xhs - img="" elementtiming="card - exposed" style="width: 100%; height: 100%; object - fit: cover;">`, should be crawled.
    - The element with `data - v - aed4aacc` is the comment, which needs to be crawled into the local JSON.
6. After all elements of the current post are captured, according to the `noimage` setting of the task, if `noimage` is `true`, skip the OCR process. If `noimage` is `false`, perform text extraction via `easyocr`, merge the content into the JSON of the current post, and then simulate pressing the `Esc` key to exit the current post.
7. When the content of all posts is crawled, iterate through all folders under the task folder, merge the JSON files in them, create a file with the task folder name in the root directory of the task folder, with the merged content and the file extension `.txt`. If there is an `--export` parameter, copy this file to the directory specified after `export` and rename it to `.md`. 